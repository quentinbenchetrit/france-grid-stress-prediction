{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis — France Grid Stress Prediction (Day-Ahead, D+1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook objectives\n",
        "\n",
        "\n",
        "- Understand the temporal structure of national electricity load and generation  \n",
        "- Validate data integrity (time axis, missingness, DST, outliers)  \n",
        "- Analyze demand-side (consumption) and supply-side (production) separately  \n",
        "- Translate EDA findings into clear modeling decisions for day-ahead forecasting  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Executive overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e92d25",
      "metadata": {},
      "source": [
        "Ensuring the Project Root as the Working Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58342294",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: /home/onyxia/work/france-grid-stress-prediction\n"
          ]
        }
      ],
      "source": [
        "import os  # OS utilities for working with directories\n",
        "\n",
        "# If the notebook is executed from the notebooks/ folder,\n",
        "# move up one level to set the project root as the working directory\n",
        "if os.getcwd().endswith(\"notebooks\"):\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "# Print the current working directory for verification\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3ef168",
      "metadata": {},
      "source": [
        "### Note on data format and preprocessing choices\n",
        "\n",
        "Some historical RTE datasets are distributed as large Excel (`.xls`) files with\n",
        "heterogeneous and partially non-tabular structures. This is notably the case for:\n",
        "\n",
        "- historical electricity consumption files  \n",
        "  (e.g. `Historique_consommation_INST_1997.xls`),\n",
        "- historical production realisation files  \n",
        "  (e.g. `RealisationDonneesProduction_2015.xls`).\n",
        "\n",
        "These Excel files may contain empty rows, repeated headers, merged cells, or\n",
        "ambiguous time encodings, which makes direct exploratory analysis fragile and\n",
        "hard to reproduce.\n",
        "\n",
        "To ensure **robustness, clarity, and reproducibility**, these datasets were first\n",
        "converted into clean, flat CSV files using **two dedicated Python preprocessing scripts**:\n",
        "one for consumption data and one for production realisation data.\n",
        "\n",
        "This conversion step is strictly a **format normalization and light cleaning stage**\n",
        "(no aggregation or feature engineering), and preserves the original information\n",
        "content of the source files.\n",
        "\n",
        "All exploratory analyses presented in this notebook are therefore conducted on\n",
        "the CSV representations derived from the original Excel sources, allowing the EDA\n",
        "to focus on data understanding rather than Excel-specific parsing issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.1 Dataset snapshot\n",
        "\n",
        "This section provides a high-level snapshot of the dataset used for exploratory analysis.\n",
        "\n",
        "The objective is to:\n",
        "- clearly identify the **time coverage**,\n",
        "- infer the **native temporal frequency**,\n",
        "- report the **size and structure** of the dataset,\n",
        "- automatically detect the **main variable families** (load, weather, generation).\n",
        "\n",
        "This snapshot acts as a reference point for all subsequent analyses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "2e030b35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_csv_or_excel(path: Path) -> pd.DataFrame:\n",
        "    if path.suffix.lower() in [\".xls\", \".xlsx\"]:\n",
        "        return pd.read_excel(path)\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def parse_dt(df: pd.DataFrame, col: str) -> pd.Series:\n",
        "    return pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "\n",
        "def infer_native_resolution(\n",
        "    df: pd.DataFrame,\n",
        "    dt_col: str,\n",
        "    group_cols: list[str] | None = None,\n",
        "    ignore_zero: bool = True\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Infers the native time resolution by taking the most common positive delta\n",
        "    within each series (group), then aggregating.\n",
        "\n",
        "    Returns: {resolution_seconds, resolution_label, diagnostics}\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    d[dt_col] = parse_dt(d, dt_col)\n",
        "    d = d.dropna(subset=[dt_col])\n",
        "\n",
        "    # Sort and compute deltas within each series\n",
        "    if group_cols:\n",
        "        d = d.sort_values(group_cols + [dt_col])\n",
        "        deltas = (\n",
        "            d.groupby(group_cols)[dt_col]\n",
        "             .diff()\n",
        "             .dt.total_seconds()\n",
        "        )\n",
        "    else:\n",
        "        d = d.sort_values(dt_col)\n",
        "        deltas = d[dt_col].diff().dt.total_seconds()\n",
        "\n",
        "    deltas = deltas.dropna()\n",
        "\n",
        "    if ignore_zero:\n",
        "        deltas = deltas[deltas > 0]\n",
        "\n",
        "    if len(deltas) == 0:\n",
        "        return {\n",
        "            \"resolution_seconds\": None,\n",
        "            \"resolution_label\": \"Unknown (no positive deltas)\",\n",
        "            \"diagnostics\": {\"n_deltas\": 0}\n",
        "        }\n",
        "\n",
        "    # Most common delta (mode)\n",
        "    vc = deltas.round().astype(\"int64\").value_counts()\n",
        "    res_s = int(vc.index[0])\n",
        "\n",
        "    def label(sec: int) -> str:\n",
        "        if sec == 900:  return \"15-minute\"\n",
        "        if sec == 1800: return \"30-minute\"\n",
        "        if sec == 3600: return \"Hourly\"\n",
        "        if sec % 3600 == 0: return f\"{sec//3600}-hour\"\n",
        "        if sec % 60 == 0:   return f\"{sec//60}-minute\"\n",
        "        return f\"{sec}-second\"\n",
        "\n",
        "    return {\n",
        "        \"resolution_seconds\": res_s,\n",
        "        \"resolution_label\": label(res_s),\n",
        "        \"diagnostics\": {\n",
        "            \"top_deltas_seconds\": vc.head(5).to_dict(),\n",
        "            \"n_deltas_used\": int(len(deltas))\n",
        "        }\n",
        "    }\n",
        "\n",
        "def detect_key_variables(columns: list[str]) -> dict[str, list[str]]:\n",
        "    mapping = {\n",
        "        \"load / consumption\": [\"consumption\", \"conso\", \"load\"],\n",
        "        \"temperature\": [\"temp\", \"temperature\"],\n",
        "        \"wind\": [\"wind\", \"eolien\"],\n",
        "        \"solar\": [\"solar\", \"pv\", \"radiation\"],\n",
        "        \"nuclear\": [\"nuclear\"],\n",
        "        \"hydro\": [\"hydro\"],\n",
        "        \"thermal\": [\"thermal\", \"gas\", \"coal\"],\n",
        "        \"installed capacity\": [\"installed\", \"capacity\", \"capacite\"],\n",
        "    }\n",
        "    out = {}\n",
        "    for group, kws in mapping.items():\n",
        "        hits = [c for c in columns if any(kw in c.lower() for kw in kws)]\n",
        "        if hits:\n",
        "            out[group] = hits\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61895fda",
      "metadata": {},
      "source": [
        "### 0.1.1 Weather — 32 cities (historical archive)\n",
        "\n",
        "This subsection provides a snapshot of the **raw weather data** extracted from the Open-Meteo\n",
        "historical archive for 32 representative French agglomerations.\n",
        "\n",
        "The analysis is performed on a **single representative yearly file**, as all years share\n",
        "the same structure and temporal resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "971fda45",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Dataset': 'Weather (32 cities)',\n",
              " 'File': '/home/onyxia/work/france-grid-stress-prediction/data/raw/weather/weather_32_cities_2015.csv',\n",
              " 'Start': Timestamp('2015-01-01 00:00:00+0000', tz='UTC'),\n",
              " 'End': Timestamp('2015-12-31 23:00:00+0000', tz='UTC'),\n",
              " 'Resolution': 'Hourly',\n",
              " 'Rows': 280320,\n",
              " 'Columns': 7,\n",
              " 'Cities': 32,\n",
              " 'Key variables': {'temperature': ['temperature_2m'],\n",
              "  'wind': ['wind_speed_10m'],\n",
              "  'solar': ['direct_radiation', 'diffuse_radiation']},\n",
              " 'Diagnostics': {'top_deltas_seconds': {3600: 280288},\n",
              "  'n_deltas_used': 280288}}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH_WEATHER = Path(\"/home/onyxia/work/france-grid-stress-prediction/data/raw/weather/weather_32_cities_2015.csv\")\n",
        "# or: \".../weather_32_cities_meteofrance_7past_7future.csv\"\n",
        "\n",
        "dfw = load_csv_or_excel(PATH_WEATHER)\n",
        "\n",
        "freq = infer_native_resolution(dfw, dt_col=\"date\", group_cols=[\"city\"])\n",
        "\n",
        "snapshot_weather = {\n",
        "    \"Dataset\": \"Weather (32 cities)\",\n",
        "    \"File\": str(PATH_WEATHER),\n",
        "    \"Start\": parse_dt(dfw, \"date\").min(),\n",
        "    \"End\": parse_dt(dfw, \"date\").max(),\n",
        "    \"Resolution\": freq[\"resolution_label\"],\n",
        "    \"Rows\": len(dfw),\n",
        "    \"Columns\": len(dfw.columns),\n",
        "    \"Cities\": int(dfw[\"city\"].nunique()),\n",
        "    \"Key variables\": detect_key_variables(list(dfw.columns)),\n",
        "    \"Diagnostics\": freq[\"diagnostics\"],\n",
        "}\n",
        "snapshot_weather\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5092c31f",
      "metadata": {},
      "source": [
        "### 0.1.2 Weather — 7 past days + 7 forecast days (Météo-France)\n",
        "\n",
        "This dataset combines **recent historical observations and short-term forecasts**\n",
        "from the Météo-France Open-Meteo endpoint.\n",
        "\n",
        "It is designed for **near real-time Day-Ahead forecasting**, not for long-term training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "96ac29c6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Dataset': 'Weather (32 cities)',\n",
              " 'File': '/home/onyxia/work/france-grid-stress-prediction/data/raw/weather/weather_32_cities_meteofrance_7past_7future.csv',\n",
              " 'Start': Timestamp('2025-12-21 00:00:00+0000', tz='UTC'),\n",
              " 'End': Timestamp('2026-01-03 23:00:00+0000', tz='UTC'),\n",
              " 'Resolution': 'Hourly',\n",
              " 'Rows': 10752,\n",
              " 'Columns': 7,\n",
              " 'Cities': 32,\n",
              " 'Key variables': {'temperature': ['temperature_2m'],\n",
              "  'wind': ['wind_speed_10m'],\n",
              "  'solar': ['direct_radiation', 'diffuse_radiation']},\n",
              " 'Diagnostics': {'top_deltas_seconds': {3600: 10720}, 'n_deltas_used': 10720}}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH_WEATHER = Path(\"/home/onyxia/work/france-grid-stress-prediction/data/raw/weather/weather_32_cities_meteofrance_7past_7future.csv\")\n",
        "\n",
        "dfw = load_csv_or_excel(PATH_WEATHER)\n",
        "\n",
        "freq = infer_native_resolution(dfw, dt_col=\"date\", group_cols=[\"city\"])\n",
        "\n",
        "snapshot_weather = {\n",
        "    \"Dataset\": \"Weather (32 cities)\",\n",
        "    \"File\": str(PATH_WEATHER),\n",
        "    \"Start\": parse_dt(dfw, \"date\").min(),\n",
        "    \"End\": parse_dt(dfw, \"date\").max(),\n",
        "    \"Resolution\": freq[\"resolution_label\"],\n",
        "    \"Rows\": len(dfw),\n",
        "    \"Columns\": len(dfw.columns),\n",
        "    \"Cities\": int(dfw[\"city\"].nunique()),\n",
        "    \"Key variables\": detect_key_variables(list(dfw.columns)),\n",
        "    \"Diagnostics\": freq[\"diagnostics\"],\n",
        "}\n",
        "snapshot_weather\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5837485",
      "metadata": {},
      "source": [
        "### 0.1.3 — Electricity consumption (RTE)\n",
        "\n",
        "The snapshot is computed on one **representative yearly file** produced by the preprocessing pipeline\n",
        "(`data/interim/consommation/consommation_<year>_long.csv`), which already follows a clean *long* time-series format\n",
        "(one row per timestamp).\n",
        "\n",
        "Because RTE historical files may contain daylight-saving irregularities or missing timestamps,\n",
        "the time resolution is **not inferred using pandas built-in frequency detection**.\n",
        "Instead, we compute the **modal time delta** between consecutive timestamps, which provides a robust estimate\n",
        "of the effective resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ee2274d2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>rows</th>\n",
              "      <th>cols</th>\n",
              "      <th>resolution_mode_min</th>\n",
              "      <th>resolution_quality_share</th>\n",
              "      <th>resolution_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>consommation_1996_long.csv</td>\n",
              "      <td>1996-01-01</td>\n",
              "      <td>1996-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>consommation_1997_long.csv</td>\n",
              "      <td>1997-01-01</td>\n",
              "      <td>1997-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>consommation_1998_long.csv</td>\n",
              "      <td>1998-01-01</td>\n",
              "      <td>1998-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>consommation_1999_long.csv</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>1999-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>consommation_2000_long.csv</td>\n",
              "      <td>2000-01-01</td>\n",
              "      <td>2000-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>consommation_2001_long.csv</td>\n",
              "      <td>2001-01-01</td>\n",
              "      <td>2001-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>consommation_2002_long.csv</td>\n",
              "      <td>2002-01-01</td>\n",
              "      <td>2002-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>consommation_2003_long.csv</td>\n",
              "      <td>2003-01-01</td>\n",
              "      <td>2003-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>consommation_2004_long.csv</td>\n",
              "      <td>2004-01-01</td>\n",
              "      <td>2004-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>consommation_2005_long.csv</td>\n",
              "      <td>2005-01-01</td>\n",
              "      <td>2005-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>consommation_2006_long.csv</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2006-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>consommation_2007_long.csv</td>\n",
              "      <td>2007-01-01</td>\n",
              "      <td>2007-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>consommation_2008_long.csv</td>\n",
              "      <td>2008-01-01</td>\n",
              "      <td>2008-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>consommation_2009_long.csv</td>\n",
              "      <td>2009-01-01</td>\n",
              "      <td>2009-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>consommation_2010_long.csv</td>\n",
              "      <td>2010-01-01</td>\n",
              "      <td>2010-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>consommation_2011_long.csv</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>2011-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>consommation_2012_long.csv</td>\n",
              "      <td>2012-01-01</td>\n",
              "      <td>2012-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>consommation_2013_long.csv</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>consommation_2014_long.csv</td>\n",
              "      <td>2014-01-01</td>\n",
              "      <td>2014-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>consommation_2015_long.csv</td>\n",
              "      <td>2015-01-01</td>\n",
              "      <td>2015-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>consommation_2016_long.csv</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>2016-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>consommation_2017_long.csv</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>consommation_2018_long.csv</td>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>consommation_2019_long.csv</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>2019-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>consommation_2020_long.csv</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>2020-10-31 23:30:00</td>\n",
              "      <td>7248</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.999862</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>consommation_2021_long.csv</td>\n",
              "      <td>2021-01-01</td>\n",
              "      <td>2021-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>consommation_2022_long.csv</td>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>2022-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>consommation_2023_long.csv</td>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>2023-12-31 23:30:00</td>\n",
              "      <td>17520</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>consommation_2024_long.csv</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>2024-12-31 23:30:00</td>\n",
              "      <td>17568</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>consommation_2025_long.csv</td>\n",
              "      <td>2025-01-01</td>\n",
              "      <td>2025-09-30 23:30:00</td>\n",
              "      <td>13104</td>\n",
              "      <td>6</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30 min</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          file      start                 end   rows  cols  \\\n",
              "0   consommation_1996_long.csv 1996-01-01 1996-12-31 23:30:00  17568     6   \n",
              "1   consommation_1997_long.csv 1997-01-01 1997-12-31 23:30:00  17520     6   \n",
              "2   consommation_1998_long.csv 1998-01-01 1998-12-31 23:30:00  17520     6   \n",
              "3   consommation_1999_long.csv 1999-01-01 1999-12-31 23:30:00  17520     6   \n",
              "4   consommation_2000_long.csv 2000-01-01 2000-12-31 23:30:00  17568     6   \n",
              "5   consommation_2001_long.csv 2001-01-01 2001-12-31 23:30:00  17520     6   \n",
              "6   consommation_2002_long.csv 2002-01-01 2002-12-31 23:30:00  17520     6   \n",
              "7   consommation_2003_long.csv 2003-01-01 2003-12-31 23:30:00  17520     6   \n",
              "8   consommation_2004_long.csv 2004-01-01 2004-12-31 23:30:00  17568     6   \n",
              "9   consommation_2005_long.csv 2005-01-01 2005-12-31 23:30:00  17520     6   \n",
              "10  consommation_2006_long.csv 2006-01-01 2006-12-31 23:30:00  17520     6   \n",
              "11  consommation_2007_long.csv 2007-01-01 2007-12-31 23:30:00  17520     6   \n",
              "12  consommation_2008_long.csv 2008-01-01 2008-12-31 23:30:00  17568     6   \n",
              "13  consommation_2009_long.csv 2009-01-01 2009-12-31 23:30:00  17520     6   \n",
              "14  consommation_2010_long.csv 2010-01-01 2010-12-31 23:30:00  17520     6   \n",
              "15  consommation_2011_long.csv 2011-01-01 2011-12-31 23:30:00  17520     6   \n",
              "16  consommation_2012_long.csv 2012-01-01 2012-12-31 23:30:00  17568     6   \n",
              "17  consommation_2013_long.csv 2013-01-01 2013-12-31 23:30:00  17520     6   \n",
              "18  consommation_2014_long.csv 2014-01-01 2014-12-31 23:30:00  17520     6   \n",
              "19  consommation_2015_long.csv 2015-01-01 2015-12-31 23:30:00  17520     6   \n",
              "20  consommation_2016_long.csv 2016-01-01 2016-12-31 23:30:00  17568     6   \n",
              "21  consommation_2017_long.csv 2017-01-01 2017-12-31 23:30:00  17520     6   \n",
              "22  consommation_2018_long.csv 2018-01-01 2018-12-31 23:30:00  17520     6   \n",
              "23  consommation_2019_long.csv 2019-01-01 2019-12-31 23:30:00  17520     6   \n",
              "24  consommation_2020_long.csv 2020-01-01 2020-10-31 23:30:00   7248     6   \n",
              "25  consommation_2021_long.csv 2021-01-01 2021-12-31 23:30:00  17520     6   \n",
              "26  consommation_2022_long.csv 2022-01-01 2022-12-31 23:30:00  17520     6   \n",
              "27  consommation_2023_long.csv 2023-01-01 2023-12-31 23:30:00  17520     6   \n",
              "28  consommation_2024_long.csv 2024-01-01 2024-12-31 23:30:00  17568     6   \n",
              "29  consommation_2025_long.csv 2025-01-01 2025-09-30 23:30:00  13104     6   \n",
              "\n",
              "    resolution_mode_min  resolution_quality_share resolution_label  \n",
              "0                  30.0                  1.000000           30 min  \n",
              "1                  30.0                  1.000000           30 min  \n",
              "2                  30.0                  1.000000           30 min  \n",
              "3                  30.0                  1.000000           30 min  \n",
              "4                  30.0                  1.000000           30 min  \n",
              "5                  30.0                  1.000000           30 min  \n",
              "6                  30.0                  1.000000           30 min  \n",
              "7                  30.0                  1.000000           30 min  \n",
              "8                  30.0                  1.000000           30 min  \n",
              "9                  30.0                  1.000000           30 min  \n",
              "10                 30.0                  1.000000           30 min  \n",
              "11                 30.0                  1.000000           30 min  \n",
              "12                 30.0                  1.000000           30 min  \n",
              "13                 30.0                  1.000000           30 min  \n",
              "14                 30.0                  1.000000           30 min  \n",
              "15                 30.0                  1.000000           30 min  \n",
              "16                 30.0                  1.000000           30 min  \n",
              "17                 30.0                  1.000000           30 min  \n",
              "18                 30.0                  1.000000           30 min  \n",
              "19                 30.0                  1.000000           30 min  \n",
              "20                 30.0                  1.000000           30 min  \n",
              "21                 30.0                  1.000000           30 min  \n",
              "22                 30.0                  1.000000           30 min  \n",
              "23                 30.0                  1.000000           30 min  \n",
              "24                 30.0                  0.999862           30 min  \n",
              "25                 30.0                  1.000000           30 min  \n",
              "26                 30.0                  1.000000           30 min  \n",
              "27                 30.0                  1.000000           30 min  \n",
              "28                 30.0                  1.000000           30 min  \n",
              "29                 30.0                  1.000000           30 min  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# Optional: Snapshot across ALL consumption long CSV files\n",
        "# (1996-2025) in data/interim/consommation\n",
        "# ==========================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CONS_DIR = Path(\"/home/onyxia/work/france-grid-stress-prediction/data/interim/consommation\")\n",
        "files = sorted(CONS_DIR.glob(\"consommation_*_long.csv\"))\n",
        "\n",
        "def resolution_mode_minutes(dt: pd.Series) -> tuple[str, float | None, float | None]:\n",
        "    dt = pd.to_datetime(dt, errors=\"coerce\")\n",
        "    dt = dt.dropna().sort_values()\n",
        "    if len(dt) < 2:\n",
        "        return (\"Unknown\", None, None)\n",
        "    delta = dt.diff().dropna()\n",
        "    mins = (delta.dt.total_seconds() / 60).round(6)\n",
        "    if mins.empty:\n",
        "        return (\"Unknown\", None, None)\n",
        "    mode = float(mins.mode().iloc[0])\n",
        "    share = float((mins == mode).mean())\n",
        "    return (f\"{mode:g} min\", mode, share)\n",
        "\n",
        "rows = []\n",
        "for p in files:\n",
        "    try:\n",
        "        df = pd.read_csv(p, usecols=lambda c: c.lower() in (\"datetime\", \"dt\", \"timestamp\", \"date\"))\n",
        "    except Exception:\n",
        "        df = pd.read_csv(p)\n",
        "\n",
        "    # datetime col detection\n",
        "    dt_col = None\n",
        "    for c in df.columns:\n",
        "        if c.lower() in (\"datetime\", \"dt\", \"timestamp\"):\n",
        "            dt_col = c\n",
        "            break\n",
        "    if dt_col is None:\n",
        "        # fallback: first column\n",
        "        dt_col = df.columns[0]\n",
        "\n",
        "    dt = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
        "    start = dt.min()\n",
        "    end = dt.max()\n",
        "\n",
        "    # get file shape cheaply (read only header + count lines)\n",
        "    # (fallback: full read if needed)\n",
        "    try:\n",
        "        full = pd.read_csv(p)\n",
        "        n_rows, n_cols = full.shape\n",
        "        cols = full.columns.tolist()\n",
        "    except Exception:\n",
        "        n_rows, n_cols = (None, None)\n",
        "        cols = []\n",
        "\n",
        "    res_label, res_mode, res_share = resolution_mode_minutes(dt)\n",
        "\n",
        "    rows.append({\n",
        "        \"file\": p.name,\n",
        "        \"start\": start,\n",
        "        \"end\": end,\n",
        "        \"rows\": n_rows,\n",
        "        \"cols\": n_cols,\n",
        "        \"resolution_mode_min\": res_mode,\n",
        "        \"resolution_quality_share\": res_share,\n",
        "        \"resolution_label\": res_label,\n",
        "    })\n",
        "\n",
        "snapshot_all = pd.DataFrame(rows).sort_values(\"file\").reset_index(drop=True)\n",
        "snapshot_all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abe0fa9",
      "metadata": {},
      "source": [
        "### 0.1.4 Dataset snapshot — RTE realised consumption (Year-To-Date)\n",
        "\n",
        "In addition to the historical consumption archives, the project relies on a **near real-time realised consumption feed**\n",
        "retrieved via the RTE API and stored as a CSV file.\n",
        "\n",
        "This Year-To-Date (YTD) dataset is used to monitor **recent consumption dynamics**, validate the forecasting pipeline\n",
        "close to real time, and ensure continuity between historical data and **day-ahead operational forecasting**.\n",
        "\n",
        "Given its **continuous updates** and **interval-based time structure**, a dedicated snapshot is performed to assess\n",
        "the **temporal coverage**, infer the **effective time resolution**, and verify structural consistency with the\n",
        "historical consumption datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "1e12e024",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'file': '/home/onyxia/work/france-grid-stress-prediction/data/raw/consommation/2025/rte_consumption_realised_ytd.csv',\n",
              " 'rows': 34398,\n",
              " 'coverage_utc': '2024-12-31 23:00:00+00:00 -> 2025-12-27 23:00:00+00:00',\n",
              " 'dominant_interval_min': 15.0,\n",
              " 'interval_stats_min (mean/median/std/min/max)': (15.0,\n",
              "  15.0,\n",
              "  0.4575156612435953,\n",
              "  -45.0,\n",
              "  75.0),\n",
              " 'unique_start_points': 34397,\n",
              " 'expected_points_from_span': 34656,\n",
              " 'missing_grid_points (count/share)': (259, '0.75%'),\n",
              " 'duplicates_on_start_date': 1,\n",
              " 'gaps_gt_dominant (count)': 142,\n",
              " 'max_gap_between_starts_min': 1470.0,\n",
              " 'bad_tiling_end_vs_next_start (count)': 143,\n",
              " 'max_tiling_mismatch_seconds': 87300.0,\n",
              " 'top_resolutions':        count     share\n",
              " -45.0      1  0.000029\n",
              "  15.0  34396  0.999942\n",
              "  75.0      1  0.000029,\n",
              " 'value_col': 'consumption_mw',\n",
              " 'null_values': 0,\n",
              " 'negative_values': 0,\n",
              " 'consumption_min/median/max_mw': (29044.0, 47528.0, 86918.0)}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "PATH = \"/home/onyxia/work/france-grid-stress-prediction/data/raw/consommation/2025/rte_consumption_realised_ytd.csv\"\n",
        "\n",
        "# --- Load ---\n",
        "df = pd.read_csv(PATH)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# --- Validate schema ---\n",
        "expected = {\"start_date\", \"end_date\"}\n",
        "missing = expected - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns {missing}. Available columns: {list(df.columns)}\")\n",
        "\n",
        "# --- Parse datetimes (keep tz -> convert to UTC for clean diffs) ---\n",
        "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors=\"coerce\", utc=True)\n",
        "df[\"end_date\"]   = pd.to_datetime(df[\"end_date\"], errors=\"coerce\", utc=True)\n",
        "if \"updated_date\" in df.columns:\n",
        "    df[\"updated_date\"] = pd.to_datetime(df[\"updated_date\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "# --- Interval duration (minutes) ---\n",
        "interval_td = df[\"end_date\"] - df[\"start_date\"]\n",
        "interval_min = interval_td.dt.total_seconds() / 60\n",
        "interval_min_clean = interval_min.dropna()\n",
        "\n",
        "# --- Resolution stats ---\n",
        "mode_delta = float(interval_min_clean.mode().iloc[0])\n",
        "mean_delta = float(interval_min_clean.mean())\n",
        "median_delta = float(interval_min_clean.median())\n",
        "std_delta = float(interval_min_clean.std())\n",
        "\n",
        "resolution_table = (\n",
        "    interval_min_clean.round(6)\n",
        "    .value_counts()\n",
        "    .sort_index()\n",
        "    .rename(\"count\")\n",
        "    .to_frame()\n",
        ")\n",
        "resolution_table[\"share\"] = resolution_table[\"count\"] / resolution_table[\"count\"].sum()\n",
        "\n",
        "# --- Coverage & grid checks ---\n",
        "start_min = df[\"start_date\"].min()\n",
        "end_max   = df[\"end_date\"].max()\n",
        "\n",
        "span_minutes = (end_max - start_min).total_seconds() / 60\n",
        "expected_points = int(round(span_minutes / mode_delta)) if span_minutes >= 0 else 0\n",
        "\n",
        "unique_starts = df[\"start_date\"].nunique(dropna=True)\n",
        "missing_points = expected_points - unique_starts\n",
        "missing_share = (missing_points / expected_points) if expected_points > 0 else np.nan\n",
        "\n",
        "# gaps between consecutive start timestamps\n",
        "dt_sorted = df[\"start_date\"].dropna().sort_values()\n",
        "gaps_min = dt_sorted.diff().dt.total_seconds().div(60).dropna()\n",
        "n_big_gaps = int((gaps_min > mode_delta + 1e-9).sum())\n",
        "max_gap = float(gaps_min.max()) if len(gaps_min) else np.nan\n",
        "\n",
        "# duplicates on start_date\n",
        "dupe_start = int(df.duplicated(subset=[\"start_date\"]).sum())\n",
        "\n",
        "# tiling check: end_date should equal next start_date (perfect chaining)\n",
        "s = df.sort_values(\"start_date\").reset_index(drop=True)\n",
        "tiling_sec = (s[\"end_date\"] - s[\"start_date\"].shift(-1)).dt.total_seconds().dropna()\n",
        "n_bad_tiling = int((tiling_sec.abs() > 1e-6).sum())\n",
        "max_tiling_mismatch_sec = float(tiling_sec.abs().max()) if len(tiling_sec) else 0.0\n",
        "\n",
        "# --- Value sanity ---\n",
        "value_col = \"consumption_mw\" if \"consumption_mw\" in df.columns else None\n",
        "if value_col:\n",
        "    null_values = int(df[value_col].isna().sum())\n",
        "    neg_values  = int((df[value_col] < 0).sum())\n",
        "    vmin, vmed, vmax = (float(df[value_col].min()),\n",
        "                        float(df[value_col].median()),\n",
        "                        float(df[value_col].max()))\n",
        "else:\n",
        "    null_values = neg_values = vmin = vmed = vmax = None\n",
        "\n",
        "# --- Summary object (compact but rich) ---\n",
        "summary = {\n",
        "    \"file\": PATH,\n",
        "    \"rows\": int(len(df)),\n",
        "    \"coverage_utc\": f\"{start_min} -> {end_max}\",\n",
        "    \"dominant_interval_min\": mode_delta,\n",
        "    \"interval_stats_min (mean/median/std/min/max)\": (\n",
        "        mean_delta,\n",
        "        median_delta,\n",
        "        std_delta,\n",
        "        float(interval_min_clean.min()),\n",
        "        float(interval_min_clean.max()),\n",
        "    ),\n",
        "    \"unique_start_points\": int(unique_starts),\n",
        "    \"expected_points_from_span\": int(expected_points),\n",
        "    \"missing_grid_points (count/share)\": (int(missing_points), f\"{missing_share:.2%}\"),\n",
        "    \"duplicates_on_start_date\": int(dupe_start),\n",
        "    \"gaps_gt_dominant (count)\": int(n_big_gaps),\n",
        "    \"max_gap_between_starts_min\": max_gap,\n",
        "    \"bad_tiling_end_vs_next_start (count)\": int(n_bad_tiling),\n",
        "    \"max_tiling_mismatch_seconds\": max_tiling_mismatch_sec,\n",
        "    \"top_resolutions\": resolution_table.head(10),\n",
        "}\n",
        "\n",
        "if value_col:\n",
        "    summary.update({\n",
        "        \"value_col\": value_col,\n",
        "        \"null_values\": null_values,\n",
        "        \"negative_values\": neg_values,\n",
        "        \"consumption_min/median/max_mw\": (vmin, vmed, vmax),\n",
        "    })\n",
        "\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd903c4",
      "metadata": {},
      "source": [
        "### 0.1.5 RTE realised generation mix (processed long format)\n",
        "\n",
        "The production “realisation” files are **standardised long-format CSVs** generated from RTE yearly raw exports\n",
        "(one file per year, one row per *(datetime × technology)*).\n",
        "\n",
        "We snapshot these processed files to verify:\n",
        "- **coverage** (min/max datetime, number of days),\n",
        "- **time granularity** (mode/median interval, missing grid points, irregularities),\n",
        "- **structural integrity** (duplicates on *(datetime, technology)*, nulls),\n",
        "- **content sanity** (number of technologies, presence of `Total`, NaN/negative `value_mw`).\n",
        "\n",
        "This ensures downstream aggregation (by technology, by day/hour) and forecasting features are built on a clean,\n",
        "consistent production panel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5ba52d98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2015_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2015-01-01 00:00:00  →  2015-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=20\n",
            "- value_mw: NaN=20 | negative=3,976\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2016_long.csv\n",
            "- rows: 105,408 | days: 366 | techs: 12 | has_total=True\n",
            "- coverage: 2016-01-01 00:00:00  →  2016-12-31 23:00:00  (~365.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,624 | dup(datetime,technology)=0 | null_cells=96\n",
            "- value_mw: NaN=96 | negative=4,798\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2017_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2017-01-01 00:00:00  →  2017-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=26\n",
            "- value_mw: NaN=26 | negative=4,557\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2018_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2018-01-01 00:00:00  →  2018-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=24\n",
            "- value_mw: NaN=24 | negative=4,681\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2019_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2019-01-01 00:00:00  →  2019-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=24\n",
            "- value_mw: NaN=24 | negative=4,521\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2020_long.csv\n",
            "- rows: 105,408 | days: 366 | techs: 12 | has_total=True\n",
            "- coverage: 2020-01-01 00:00:00  →  2020-12-31 23:00:00  (~365.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,624 | dup(datetime,technology)=0 | null_cells=108\n",
            "- value_mw: NaN=108 | negative=4,317\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2021_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2021-01-01 00:00:00  →  2021-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=12\n",
            "- value_mw: NaN=12 | negative=4,021\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2022_long.csv\n",
            "- rows: 105,120 | days: 365 | techs: 12 | has_total=True\n",
            "- coverage: 2022-01-01 00:00:00  →  2022-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (91.7%), median=0, mean=5.00, irregular=100.00%\n",
            "- integrity: dup_ts=96,360 | dup(datetime,technology)=0 | null_cells=12\n",
            "- value_mw: NaN=12 | negative=4,311\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2023_long.csv\n",
            "- rows: 108,816 | days: 365 | techs: 13 | has_total=True\n",
            "- coverage: 2023-01-01 00:00:00  →  2023-12-31 23:00:00  (~364.96 days)\n",
            "- granularity (min): mode=0 (92.0%), median=0, mean=4.83, irregular=100.00%\n",
            "- integrity: dup_ts=100,056 | dup(datetime,technology)=0 | null_cells=740\n",
            "- value_mw: NaN=740 | negative=3,997\n",
            "\n",
            "============================================================================================\n",
            "File: production_realisation_2024_long.csv\n",
            "- rows: 114,192 | days: 366 | techs: 13 | has_total=True\n",
            "- coverage: 2024-01-01 00:00:00  →  2024-12-31 23:00:00  (~365.96 days)\n",
            "- granularity (min): mode=0 (92.3%), median=0, mean=4.61, irregular=100.00%\n",
            "- integrity: dup_ts=105,408 | dup(datetime,technology)=0 | null_cells=47\n",
            "- value_mw: NaN=47 | negative=3,951\n",
            "\n",
            "--------------------------------------------------------------------------------------------\n",
            "Processed generation mix snapshot: 10 file(s) scanned from /home/onyxia/work/france-grid-stress-prediction/data/interim/production\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>rows</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>span_days</th>\n",
              "      <th>mode_interval_min</th>\n",
              "      <th>mode_share</th>\n",
              "      <th>median_interval_min</th>\n",
              "      <th>mean_interval_min</th>\n",
              "      <th>missing_grid_points</th>\n",
              "      <th>...</th>\n",
              "      <th>irregular_intervals_share</th>\n",
              "      <th>dup_timestamps</th>\n",
              "      <th>dup_keys_datetime_tech</th>\n",
              "      <th>null_cells_total</th>\n",
              "      <th>n_days</th>\n",
              "      <th>n_technologies</th>\n",
              "      <th>has_total</th>\n",
              "      <th>nan_value_mw</th>\n",
              "      <th>negative_value_mw</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>production_realisation_2015_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2015-01-01</td>\n",
              "      <td>2015-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>20</td>\n",
              "      <td>3976</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>production_realisation_2016_long.csv</td>\n",
              "      <td>105408</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>2016-12-31 23:00:00</td>\n",
              "      <td>365.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999478</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96624</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>366</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>96</td>\n",
              "      <td>4798</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>production_realisation_2017_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>26</td>\n",
              "      <td>4557</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>production_realisation_2018_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>24</td>\n",
              "      <td>4681</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>production_realisation_2019_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>2019-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>24</td>\n",
              "      <td>4521</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>production_realisation_2020_long.csv</td>\n",
              "      <td>105408</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>2020-12-31 23:00:00</td>\n",
              "      <td>365.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999478</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96624</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>366</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>108</td>\n",
              "      <td>4317</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>production_realisation_2021_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2021-01-01</td>\n",
              "      <td>2021-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "      <td>4021</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>production_realisation_2022_long.csv</td>\n",
              "      <td>105120</td>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>2022-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.916675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.999477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>96360</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "      <td>4311</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>production_realisation_2023_long.csv</td>\n",
              "      <td>108816</td>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>2023-12-31 23:00:00</td>\n",
              "      <td>364.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.919506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.829665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100056</td>\n",
              "      <td>0</td>\n",
              "      <td>740</td>\n",
              "      <td>365</td>\n",
              "      <td>13</td>\n",
              "      <td>True</td>\n",
              "      <td>740</td>\n",
              "      <td>3997</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>production_realisation_2024_long.csv</td>\n",
              "      <td>114192</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>2024-12-31 23:00:00</td>\n",
              "      <td>365.958333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.614900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>105408</td>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>366</td>\n",
              "      <td>13</td>\n",
              "      <td>True</td>\n",
              "      <td>47</td>\n",
              "      <td>3951</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   file    rows      start  \\\n",
              "0  production_realisation_2015_long.csv  105120 2015-01-01   \n",
              "1  production_realisation_2016_long.csv  105408 2016-01-01   \n",
              "2  production_realisation_2017_long.csv  105120 2017-01-01   \n",
              "3  production_realisation_2018_long.csv  105120 2018-01-01   \n",
              "4  production_realisation_2019_long.csv  105120 2019-01-01   \n",
              "5  production_realisation_2020_long.csv  105408 2020-01-01   \n",
              "6  production_realisation_2021_long.csv  105120 2021-01-01   \n",
              "7  production_realisation_2022_long.csv  105120 2022-01-01   \n",
              "8  production_realisation_2023_long.csv  108816 2023-01-01   \n",
              "9  production_realisation_2024_long.csv  114192 2024-01-01   \n",
              "\n",
              "                  end   span_days  mode_interval_min  mode_share  \\\n",
              "0 2015-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "1 2016-12-31 23:00:00  365.958333                0.0    0.916675   \n",
              "2 2017-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "3 2018-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "4 2019-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "5 2020-12-31 23:00:00  365.958333                0.0    0.916675   \n",
              "6 2021-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "7 2022-12-31 23:00:00  364.958333                0.0    0.916675   \n",
              "8 2023-12-31 23:00:00  364.958333                0.0    0.919506   \n",
              "9 2024-12-31 23:00:00  365.958333                0.0    0.923085   \n",
              "\n",
              "   median_interval_min  mean_interval_min  missing_grid_points  ...  \\\n",
              "0                  0.0           4.999477                  NaN  ...   \n",
              "1                  0.0           4.999478                  NaN  ...   \n",
              "2                  0.0           4.999477                  NaN  ...   \n",
              "3                  0.0           4.999477                  NaN  ...   \n",
              "4                  0.0           4.999477                  NaN  ...   \n",
              "5                  0.0           4.999478                  NaN  ...   \n",
              "6                  0.0           4.999477                  NaN  ...   \n",
              "7                  0.0           4.999477                  NaN  ...   \n",
              "8                  0.0           4.829665                  NaN  ...   \n",
              "9                  0.0           4.614900                  NaN  ...   \n",
              "\n",
              "   irregular_intervals_share  dup_timestamps  dup_keys_datetime_tech  \\\n",
              "0                        1.0           96360                       0   \n",
              "1                        1.0           96624                       0   \n",
              "2                        1.0           96360                       0   \n",
              "3                        1.0           96360                       0   \n",
              "4                        1.0           96360                       0   \n",
              "5                        1.0           96624                       0   \n",
              "6                        1.0           96360                       0   \n",
              "7                        1.0           96360                       0   \n",
              "8                        1.0          100056                       0   \n",
              "9                        1.0          105408                       0   \n",
              "\n",
              "   null_cells_total  n_days  n_technologies  has_total  nan_value_mw  \\\n",
              "0                20     365              12       True            20   \n",
              "1                96     366              12       True            96   \n",
              "2                26     365              12       True            26   \n",
              "3                24     365              12       True            24   \n",
              "4                24     365              12       True            24   \n",
              "5               108     366              12       True           108   \n",
              "6                12     365              12       True            12   \n",
              "7                12     365              12       True            12   \n",
              "8               740     365              13       True           740   \n",
              "9                47     366              13       True            47   \n",
              "\n",
              "   negative_value_mw                                               path  \n",
              "0               3976  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "1               4798  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "2               4557  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "3               4681  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "4               4521  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "5               4317  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "6               4021  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "7               4311  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "8               3997  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "9               3951  /home/onyxia/work/france-grid-stress-predictio...  \n",
              "\n",
              "[10 rows x 21 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Snapshot — RTE realised generation mix (processed long format) ===\n",
        "# Assumes yearly processed files like: production_realisation_2017_long.csv\n",
        "# Columns: datetime,date,year,hour_interval,technology,value_mw\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers: time resolution\n",
        "# ---------------------------\n",
        "def _as_minutes(delta) -> float:\n",
        "    if pd.isna(delta):\n",
        "        return np.nan\n",
        "    td = pd.to_timedelta(delta)\n",
        "    return td.total_seconds() / 60.0\n",
        "\n",
        "def describe_time_index(dt: pd.Series) -> dict:\n",
        "    s = pd.to_datetime(dt, errors=\"coerce\").dropna().sort_values()\n",
        "    if s.empty:\n",
        "        return {\"start\": None, \"end\": None, \"span_days\": np.nan, \"mode_min\": np.nan,\n",
        "                \"mode_share\": np.nan, \"median_min\": np.nan, \"mean_min\": np.nan,\n",
        "                \"missing_grid_points\": np.nan, \"missing_share_vs_grid\": np.nan,\n",
        "                \"irregular_intervals_share\": np.nan, \"duplicates_ts\": 0}\n",
        "\n",
        "    start, end = s.iloc[0], s.iloc[-1]\n",
        "    span_days = (end - start).total_seconds() / 86400.0\n",
        "\n",
        "    duplicates_ts = int(s.duplicated().sum())\n",
        "\n",
        "    deltas = s.diff().dropna()\n",
        "    if deltas.empty:\n",
        "        return {\"start\": start, \"end\": end, \"span_days\": span_days, \"mode_min\": np.nan,\n",
        "                \"mode_share\": np.nan, \"median_min\": np.nan, \"mean_min\": np.nan,\n",
        "                \"missing_grid_points\": np.nan, \"missing_share_vs_grid\": np.nan,\n",
        "                \"irregular_intervals_share\": np.nan, \"duplicates_ts\": duplicates_ts}\n",
        "\n",
        "    minutes = deltas.map(_as_minutes)\n",
        "\n",
        "    mode_min = float(pd.Series(minutes).mode().iloc[0])\n",
        "    mode_share = float((minutes == mode_min).mean())\n",
        "    median_min = float(np.nanmedian(minutes))\n",
        "    mean_min = float(np.nanmean(minutes))\n",
        "\n",
        "    nonzero = minutes[minutes > 0]\n",
        "    if len(nonzero) == 0:\n",
        "        irregular_share = 0.0\n",
        "    else:\n",
        "        irregular_share = float((nonzero != mode_min).mean())\n",
        "\n",
        "    missing_grid_points = np.nan\n",
        "    missing_share_vs_grid = np.nan\n",
        "    if mode_share >= 0.90 and mode_min > 0:\n",
        "        freq = pd.Timedelta(minutes=mode_min)\n",
        "        grid = pd.date_range(start=start, end=end, freq=freq)\n",
        "        observed = pd.Index(s.unique())\n",
        "        missing = grid.difference(observed)\n",
        "        missing_grid_points = int(len(missing))\n",
        "        missing_share_vs_grid = float(len(missing) / len(grid)) if len(grid) else 0.0\n",
        "\n",
        "    return {\n",
        "        \"start\": start,\n",
        "        \"end\": end,\n",
        "        \"span_days\": float(span_days),\n",
        "        \"mode_min\": mode_min,\n",
        "        \"mode_share\": mode_share,\n",
        "        \"median_min\": median_min,\n",
        "        \"mean_min\": mean_min,\n",
        "        \"missing_grid_points\": missing_grid_points,\n",
        "        \"missing_share_vs_grid\": missing_share_vs_grid,\n",
        "        \"irregular_intervals_share\": irregular_share,\n",
        "        \"duplicates_ts\": duplicates_ts,\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Snapshot main\n",
        "# ---------------------------\n",
        "def snapshot_generation_mix_processed_long(\n",
        "    base_dir: str = \"/home/onyxia/work/france-grid-stress-prediction\",\n",
        "    rel_dir: str = \"data/interim/production\",\n",
        "    pattern: str = \"production_realisation_*_long.csv\",\n",
        "    print_each_file: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads all processed long-format generation mix files and prints a compact snapshot.\n",
        "    Returns a per-file summary dataframe.\n",
        "    \"\"\"\n",
        "    folder = Path(base_dir) / rel_dir\n",
        "    files = sorted(folder.glob(pattern))\n",
        "\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No files found in: {folder} (pattern: {pattern})\")\n",
        "\n",
        "    rows = []\n",
        "    for fp in files:\n",
        "        df = pd.read_csv(fp, parse_dates=[\"datetime\"])\n",
        "\n",
        "        # Coverage & granularity\n",
        "        t = describe_time_index(df[\"datetime\"])\n",
        "\n",
        "        # Structural integrity\n",
        "        n_rows = int(len(df))\n",
        "        n_null_any = int(df.isna().sum().sum())\n",
        "        n_dup_key = int(df.duplicated(subset=[\"datetime\", \"technology\"]).sum()) if {\"datetime\",\"technology\"}.issubset(df.columns) else np.nan\n",
        "\n",
        "        # Content sanity\n",
        "        tech_n = int(df[\"technology\"].nunique(dropna=True)) if \"technology\" in df.columns else np.nan\n",
        "        has_total = bool((df[\"technology\"] == \"Total\").any()) if \"technology\" in df.columns else False\n",
        "\n",
        "        v = pd.to_numeric(df[\"value_mw\"], errors=\"coerce\") if \"value_mw\" in df.columns else pd.Series([], dtype=float)\n",
        "        n_nan_v = int(v.isna().sum()) if n_rows else 0\n",
        "        n_neg_v = int((v < 0).sum()) if n_rows else 0\n",
        "\n",
        "        n_days = int(pd.to_datetime(df[\"date\"], errors=\"coerce\").nunique(dropna=True)) if \"date\" in df.columns else np.nan\n",
        "\n",
        "        row = {\n",
        "            \"file\": fp.name,\n",
        "            \"rows\": n_rows,\n",
        "            \"start\": t[\"start\"],\n",
        "            \"end\": t[\"end\"],\n",
        "            \"span_days\": t[\"span_days\"],\n",
        "            \"mode_interval_min\": t[\"mode_min\"],\n",
        "            \"mode_share\": t[\"mode_share\"],\n",
        "            \"median_interval_min\": t[\"median_min\"],\n",
        "            \"mean_interval_min\": t[\"mean_min\"],\n",
        "            \"missing_grid_points\": t[\"missing_grid_points\"],\n",
        "            \"missing_share_vs_grid\": t[\"missing_share_vs_grid\"],\n",
        "            \"irregular_intervals_share\": t[\"irregular_intervals_share\"],\n",
        "            \"dup_timestamps\": t[\"duplicates_ts\"],\n",
        "            \"dup_keys_datetime_tech\": n_dup_key,\n",
        "            \"null_cells_total\": n_null_any,\n",
        "            \"n_days\": n_days,\n",
        "            \"n_technologies\": tech_n,\n",
        "            \"has_total\": has_total,\n",
        "            \"nan_value_mw\": n_nan_v,\n",
        "            \"negative_value_mw\": n_neg_v,\n",
        "            \"path\": str(fp),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "        if print_each_file:\n",
        "            print(\"\\n\" + \"=\" * 92)\n",
        "            print(f\"File: {fp.name}\")\n",
        "            print(f\"- rows: {n_rows:,} | days: {n_days if not pd.isna(n_days) else 'NA'} | techs: {tech_n} | has_total={has_total}\")\n",
        "            print(f\"- coverage: {t['start']}  →  {t['end']}  (~{t['span_days']:.2f} days)\")\n",
        "            print(\n",
        "                f\"- granularity (min): mode={t['mode_min']:g} ({t['mode_share']:.1%}), \"\n",
        "                f\"median={t['median_min']:g}, mean={t['mean_min']:.2f}, \"\n",
        "                f\"irregular={t['irregular_intervals_share']:.2%}\"\n",
        "            )\n",
        "            if not pd.isna(t[\"missing_grid_points\"]):\n",
        "                print(\n",
        "                    f\"- grid: missing_points={int(t['missing_grid_points']):,} \"\n",
        "                    f\"({t['missing_share_vs_grid']:.2%} of expected grid)\"\n",
        "                )\n",
        "            print(f\"- integrity: dup_ts={t['duplicates_ts']:,} | dup(datetime,technology)={n_dup_key:,} | null_cells={n_null_any:,}\")\n",
        "            print(f\"- value_mw: NaN={n_nan_v:,} | negative={n_neg_v:,}\")\n",
        "\n",
        "    summary = pd.DataFrame(rows).sort_values(\"file\").reset_index(drop=True)\n",
        "\n",
        "    # Always print a final one-line recap so you \"see something\" even if print_each_file=False\n",
        "    print(\"\\n\" + \"-\" * 92)\n",
        "    print(f\"Processed generation mix snapshot: {len(summary)} file(s) scanned from {folder}\")\n",
        "    print(\"-\" * 92)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# --- Run (this is what was missing if you saw 'nothing') ---\n",
        "generation_mix_summary = snapshot_generation_mix_processed_long(\n",
        "    base_dir=\"/home/onyxia/work/france-grid-stress-prediction\",\n",
        "    rel_dir=\"data/interim/production\",\n",
        "    pattern=\"production_realisation_*_long.csv\",\n",
        "    print_each_file=True,\n",
        ")\n",
        "\n",
        "generation_mix_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58212b85",
      "metadata": {},
      "source": [
        "### 0.1.6 Dataset snapshot — RTE realised generation mix (API, 15-min, YTD)\n",
        "\n",
        "This CSV corresponds to a **near real-time Year-To-Date feed** of RTE *actual electricity generation*\n",
        "retrieved via the official API, with a **15-minute time resolution**.\n",
        "In the current setup, the dataset is restricted to **SOLAR** and **WIND** production types.\n",
        "\n",
        "A dedicated snapshot is performed to:\n",
        "- assess **data freshness** (latest `end_date` and `updated_date` relative to the current time),\n",
        "- characterise **temporal coverage** (start/end timestamps and number of covered days),\n",
        "- validate the **time grid structure** (expected 15-minute cadence, missing or irregular intervals),\n",
        "- check **structural integrity** (duplicate intervals, missing values),\n",
        "- ensure **content sanity** (NaN or negative `value_mw`, consistency across production types).\n",
        "\n",
        "This step guarantees that the most recent generation data can be safely integrated\n",
        "into the real-time forecasting and grid stress assessment pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "7e7f37ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_52691/2019166923.py:12: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  return pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
            "/tmp/ipykernel_52691/2019166923.py:12: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  return pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
            "/tmp/ipykernel_52691/2019166923.py:12: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  return pd.to_datetime(s, errors=\"coerce\", utc=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================================\n",
            "RTE realised generation mix — API (15-min, YTD) snapshot\n",
            "================================================================================================\n",
            "File: rte_actual_generation_mix_15min_ytd_solar_wind.csv\n",
            "Path: /home/onyxia/work/france-grid-stress-prediction/data/raw/production/rte_actual_generation_mix_15min_ytd_solar_wind.csv\n",
            "- rows: 67,000\n",
            "- production_types: ['SOLAR', 'WIND'] (n=2), subtypes n=1\n",
            "- coverage: 2025-01-01 00:15:00+01:00  →  2025-12-28 00:00:00+01:00  (~360.99 days)\n",
            "- freshness (Europe/Paris):\n",
            "    • lag vs latest end_date:     2077 min (~34.62 h)\n",
            "    • lag vs latest updated_date: 2089 min (~34.82 h)\n",
            "- time grid (based on end_date):\n",
            "    • interval minutes: mode=15 (100.0%), median=15, mean=37.48\n",
            "    • irregular intervals share: 0.04%\n",
            "    • missing points vs inferred grid: 20,785 (59.98%)\n",
            "- measurement window (end_date - start_date):\n",
            "    • mode=15 min (100.0%) | non-15min windows=0\n",
            "- integrity: dup_keys=0 | null_cells=0\n",
            "- value_mw: NaN=0 | negative=10\n",
            "\n",
            "Per production_type summary:\n",
            "                  rows                      start                        end  nan_value  neg_value      mean_mw  p50_mw    p95_mw  max_mw\n",
            "production_type                                                                                                                          \n",
            "SOLAR            33216  2025-01-01 00:15:00+01:00  2025-12-27 22:30:00+01:00          0          0  3630.269298   496.0  13533.75   19512\n",
            "WIND             33784  2025-01-01 00:15:00+01:00  2025-12-28 00:00:00+01:00          0         10  5512.952877  4347.0  13302.85   19597\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>path</th>\n",
              "      <th>rows</th>\n",
              "      <th>production_types</th>\n",
              "      <th>n_types</th>\n",
              "      <th>n_subtypes</th>\n",
              "      <th>coverage_start</th>\n",
              "      <th>coverage_end</th>\n",
              "      <th>span_days</th>\n",
              "      <th>interval_mode_min</th>\n",
              "      <th>...</th>\n",
              "      <th>irregular_intervals_share</th>\n",
              "      <th>freshness_lag_end_minutes</th>\n",
              "      <th>freshness_lag_updated_minutes</th>\n",
              "      <th>dup_keys</th>\n",
              "      <th>null_cells_total</th>\n",
              "      <th>nan_value_mw</th>\n",
              "      <th>negative_value_mw</th>\n",
              "      <th>window_minutes_mode</th>\n",
              "      <th>window_minutes_mode_share</th>\n",
              "      <th>non_15min_windows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rte_actual_generation_mix_15min_ytd_solar_wind...</td>\n",
              "      <td>/home/onyxia/work/france-grid-stress-predictio...</td>\n",
              "      <td>67000</td>\n",
              "      <td>[SOLAR, WIND]</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-01-01 00:15:00+01:00</td>\n",
              "      <td>2025-12-28 00:00:00+01:00</td>\n",
              "      <td>360.989583</td>\n",
              "      <td>15.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00036</td>\n",
              "      <td>2077.453745</td>\n",
              "      <td>2089.220411</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                file  \\\n",
              "0  rte_actual_generation_mix_15min_ytd_solar_wind...   \n",
              "\n",
              "                                                path   rows production_types  \\\n",
              "0  /home/onyxia/work/france-grid-stress-predictio...  67000    [SOLAR, WIND]   \n",
              "\n",
              "   n_types  n_subtypes            coverage_start              coverage_end  \\\n",
              "0        2           1 2025-01-01 00:15:00+01:00 2025-12-28 00:00:00+01:00   \n",
              "\n",
              "    span_days  interval_mode_min  ...  irregular_intervals_share  \\\n",
              "0  360.989583               15.0  ...                    0.00036   \n",
              "\n",
              "   freshness_lag_end_minutes  freshness_lag_updated_minutes  dup_keys  \\\n",
              "0                2077.453745                    2089.220411         0   \n",
              "\n",
              "   null_cells_total  nan_value_mw  negative_value_mw  window_minutes_mode  \\\n",
              "0                 0             0                 10                 15.0   \n",
              "\n",
              "   window_minutes_mode_share  non_15min_windows  \n",
              "0                        1.0                  0  \n",
              "\n",
              "[1 rows x 25 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Snapshot — RTE realised generation mix (API, 15-min, YTD) ===\n",
        "# File schema (expected):\n",
        "# production_type, production_subtype, start_date, end_date, updated_date, value_mw\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "API_FILE = Path(\"/home/onyxia/work/france-grid-stress-prediction/data/raw/production/rte_actual_generation_mix_15min_ytd_solar_wind.csv\")\n",
        "\n",
        "def _to_dt(s: pd.Series) -> pd.Series:\n",
        "    return pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
        "\n",
        "def _timedelta_minutes(td) -> float:\n",
        "    if pd.isna(td):\n",
        "        return np.nan\n",
        "    return pd.to_timedelta(td).total_seconds() / 60.0\n",
        "\n",
        "def describe_grid(times: pd.Series) -> dict:\n",
        "    \"\"\"\n",
        "    Analyse the time grid using a sorted unique timestamp series.\n",
        "    Returns: mode/median/mean interval (minutes), missing points vs inferred grid,\n",
        "    and irregular interval share.\n",
        "    \"\"\"\n",
        "    t = pd.to_datetime(times, errors=\"coerce\").dropna().drop_duplicates().sort_values()\n",
        "    if t.empty or len(t) < 2:\n",
        "        return {\n",
        "            \"t_start\": None, \"t_end\": None, \"span_days\": np.nan,\n",
        "            \"mode_min\": np.nan, \"mode_share\": np.nan,\n",
        "            \"median_min\": np.nan, \"mean_min\": np.nan,\n",
        "            \"missing_points\": np.nan, \"missing_share\": np.nan,\n",
        "            \"irregular_share\": np.nan,\n",
        "        }\n",
        "\n",
        "    deltas = t.diff().dropna()\n",
        "    mins = deltas.map(_timedelta_minutes)\n",
        "\n",
        "    mode_min = float(pd.Series(mins).mode().iloc[0])\n",
        "    mode_share = float((mins == mode_min).mean())\n",
        "    median_min = float(np.nanmedian(mins))\n",
        "    mean_min = float(np.nanmean(mins))\n",
        "\n",
        "    nonzero = mins[mins > 0]\n",
        "    irregular_share = float((nonzero != mode_min).mean()) if len(nonzero) else 0.0\n",
        "\n",
        "    t_start, t_end = t.iloc[0], t.iloc[-1]\n",
        "    span_days = float((t_end - t_start).total_seconds() / 86400.0)\n",
        "\n",
        "    missing_points = np.nan\n",
        "    missing_share = np.nan\n",
        "    # Build an expected grid only if cadence is clearly dominant\n",
        "    if mode_share >= 0.90 and mode_min > 0:\n",
        "        freq = pd.Timedelta(minutes=mode_min)\n",
        "        grid = pd.date_range(t_start, t_end, freq=freq)\n",
        "        missing = pd.Index(grid).difference(pd.Index(t))\n",
        "        missing_points = int(len(missing))\n",
        "        missing_share = float(len(missing) / len(grid)) if len(grid) else 0.0\n",
        "\n",
        "    return {\n",
        "        \"t_start\": t_start,\n",
        "        \"t_end\": t_end,\n",
        "        \"span_days\": span_days,\n",
        "        \"mode_min\": mode_min,\n",
        "        \"mode_share\": mode_share,\n",
        "        \"median_min\": median_min,\n",
        "        \"mean_min\": mean_min,\n",
        "        \"missing_points\": missing_points,\n",
        "        \"missing_share\": missing_share,\n",
        "        \"irregular_share\": irregular_share,\n",
        "    }\n",
        "\n",
        "def snapshot_rte_api_15min(file_path: Path) -> dict:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    required = {\"production_type\", \"production_subtype\", \"start_date\", \"end_date\", \"updated_date\", \"value_mw\"}\n",
        "    missing_cols = required.difference(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in {file_path.name}: {sorted(missing_cols)}. Found: {list(df.columns)}\")\n",
        "\n",
        "    # Parse timestamps (they are timezone-aware in your CSV)\n",
        "    df[\"start_date\"] = _to_dt(df[\"start_date\"])\n",
        "    df[\"end_date\"] = _to_dt(df[\"end_date\"])\n",
        "    df[\"updated_date\"] = _to_dt(df[\"updated_date\"])\n",
        "    df[\"value_mw\"] = pd.to_numeric(df[\"value_mw\"], errors=\"coerce\")\n",
        "\n",
        "    # Basic counts\n",
        "    n_rows = int(len(df))\n",
        "    n_types = int(df[\"production_type\"].nunique(dropna=True))\n",
        "    types = sorted(df[\"production_type\"].dropna().unique().tolist())\n",
        "    n_subtypes = int(df[\"production_subtype\"].nunique(dropna=True))\n",
        "\n",
        "    # Coverage: based on measurement window end_date\n",
        "    grid_info = describe_grid(df[\"end_date\"])\n",
        "\n",
        "    # Freshness: compare latest end_date and latest updated_date to \"now\" in Europe/Paris\n",
        "    now = pd.Timestamp.now(tz=\"Europe/Paris\")\n",
        "    end_max = df[\"end_date\"].max()\n",
        "    upd_max = df[\"updated_date\"].max()\n",
        "\n",
        "    # Ensure tz-aware comparisons\n",
        "    end_max_local = end_max.tz_convert(\"Europe/Paris\") if getattr(end_max, \"tzinfo\", None) else end_max.tz_localize(\"Europe/Paris\")\n",
        "    upd_max_local = upd_max.tz_convert(\"Europe/Paris\") if getattr(upd_max, \"tzinfo\", None) else upd_max.tz_localize(\"Europe/Paris\")\n",
        "\n",
        "    lag_end_min = float((now - end_max_local).total_seconds() / 60.0)\n",
        "    lag_upd_min = float((now - upd_max_local).total_seconds() / 60.0)\n",
        "\n",
        "    # Integrity\n",
        "    dup_key = int(df.duplicated(subset=[\"production_type\", \"production_subtype\", \"start_date\", \"end_date\"]).sum())\n",
        "    null_cells = int(df.isna().sum().sum())\n",
        "    nan_value = int(df[\"value_mw\"].isna().sum())\n",
        "    neg_value = int((df[\"value_mw\"] < 0).sum())\n",
        "\n",
        "    # 15-min window check: end-start should be 15 minutes almost always\n",
        "    win_min = (df[\"end_date\"] - df[\"start_date\"]).map(_timedelta_minutes)\n",
        "    win_mode = float(pd.Series(win_min.dropna()).mode().iloc[0]) if win_min.notna().any() else np.nan\n",
        "    win_mode_share = float((win_min == win_mode).mean()) if win_min.notna().any() else np.nan\n",
        "    bad_window = int((win_min != 15).sum()) if win_min.notna().any() else 0\n",
        "\n",
        "    # Per-type coverage summary (min/max and missingness of value_mw)\n",
        "    by_type = (\n",
        "        df.groupby(\"production_type\")\n",
        "          .agg(\n",
        "              rows=(\"value_mw\", \"size\"),\n",
        "              start=(\"end_date\", \"min\"),\n",
        "              end=(\"end_date\", \"max\"),\n",
        "              nan_value=(\"value_mw\", lambda s: int(pd.to_numeric(s, errors=\"coerce\").isna().sum())),\n",
        "              neg_value=(\"value_mw\", lambda s: int((pd.to_numeric(s, errors=\"coerce\") < 0).sum())),\n",
        "              mean_mw=(\"value_mw\", \"mean\"),\n",
        "              p50_mw=(\"value_mw\", \"median\"),\n",
        "              p95_mw=(\"value_mw\", lambda s: float(np.nanpercentile(s, 95)) if s.notna().any() else np.nan),\n",
        "              max_mw=(\"value_mw\", \"max\"),\n",
        "          )\n",
        "          .sort_index()\n",
        "    )\n",
        "\n",
        "    snap = {\n",
        "        \"file\": file_path.name,\n",
        "        \"path\": str(file_path),\n",
        "        \"rows\": n_rows,\n",
        "        \"production_types\": types,\n",
        "        \"n_types\": n_types,\n",
        "        \"n_subtypes\": n_subtypes,\n",
        "        \"coverage_start\": grid_info[\"t_start\"],\n",
        "        \"coverage_end\": grid_info[\"t_end\"],\n",
        "        \"span_days\": grid_info[\"span_days\"],\n",
        "        \"interval_mode_min\": grid_info[\"mode_min\"],\n",
        "        \"interval_mode_share\": grid_info[\"mode_share\"],\n",
        "        \"interval_median_min\": grid_info[\"median_min\"],\n",
        "        \"interval_mean_min\": grid_info[\"mean_min\"],\n",
        "        \"missing_grid_points\": grid_info[\"missing_points\"],\n",
        "        \"missing_grid_share\": grid_info[\"missing_share\"],\n",
        "        \"irregular_intervals_share\": grid_info[\"irregular_share\"],\n",
        "        \"freshness_lag_end_minutes\": lag_end_min,\n",
        "        \"freshness_lag_updated_minutes\": lag_upd_min,\n",
        "        \"dup_keys\": dup_key,\n",
        "        \"null_cells_total\": null_cells,\n",
        "        \"nan_value_mw\": nan_value,\n",
        "        \"negative_value_mw\": neg_value,\n",
        "        \"window_minutes_mode\": win_mode,\n",
        "        \"window_minutes_mode_share\": win_mode_share,\n",
        "        \"non_15min_windows\": bad_window,\n",
        "    }\n",
        "    return snap, by_type\n",
        "\n",
        "def print_snapshot(snap: dict, by_type: pd.DataFrame) -> None:\n",
        "    print(\"\\n\" + \"=\" * 96)\n",
        "    print(\"RTE realised generation mix — API (15-min, YTD) snapshot\")\n",
        "    print(\"=\" * 96)\n",
        "    print(f\"File: {snap['file']}\")\n",
        "    print(f\"Path: {snap['path']}\")\n",
        "    print(f\"- rows: {snap['rows']:,}\")\n",
        "    print(f\"- production_types: {snap['production_types']} (n={snap['n_types']}), subtypes n={snap['n_subtypes']}\")\n",
        "    print(f\"- coverage: {snap['coverage_start']}  →  {snap['coverage_end']}  (~{snap['span_days']:.2f} days)\")\n",
        "    print(f\"- freshness (Europe/Paris):\")\n",
        "    print(f\"    • lag vs latest end_date:     {snap['freshness_lag_end_minutes']:.0f} min (~{snap['freshness_lag_end_minutes']/60:.2f} h)\")\n",
        "    print(f\"    • lag vs latest updated_date: {snap['freshness_lag_updated_minutes']:.0f} min (~{snap['freshness_lag_updated_minutes']/60:.2f} h)\")\n",
        "    print(\"- time grid (based on end_date):\")\n",
        "    print(\n",
        "        f\"    • interval minutes: mode={snap['interval_mode_min']:g} ({snap['interval_mode_share']:.1%}), \"\n",
        "        f\"median={snap['interval_median_min']:g}, mean={snap['interval_mean_min']:.2f}\"\n",
        "    )\n",
        "    print(f\"    • irregular intervals share: {snap['irregular_intervals_share']:.2%}\")\n",
        "    if not pd.isna(snap[\"missing_grid_points\"]):\n",
        "        print(f\"    • missing points vs inferred grid: {int(snap['missing_grid_points']):,} ({snap['missing_grid_share']:.2%})\")\n",
        "    print(\"- measurement window (end_date - start_date):\")\n",
        "    print(f\"    • mode={snap['window_minutes_mode']:g} min ({snap['window_minutes_mode_share']:.1%}) | non-15min windows={snap['non_15min_windows']:,}\")\n",
        "    print(f\"- integrity: dup_keys={snap['dup_keys']:,} | null_cells={snap['null_cells_total']:,}\")\n",
        "    print(f\"- value_mw: NaN={snap['nan_value_mw']:,} | negative={snap['negative_value_mw']:,}\")\n",
        "    print(\"\\nPer production_type summary:\")\n",
        "    display_cols = [\"rows\", \"start\", \"end\", \"nan_value\", \"neg_value\", \"mean_mw\", \"p50_mw\", \"p95_mw\", \"max_mw\"]\n",
        "    print(by_type[display_cols].to_string())\n",
        "\n",
        "# --- Run ---\n",
        "snap, per_type = snapshot_rte_api_15min(API_FILE)\n",
        "print_snapshot(snap, per_type)\n",
        "\n",
        "# Optional: keep a 1-row dataframe for later concatenation\n",
        "rte_api_generation_snapshot_df = pd.DataFrame([snap])\n",
        "rte_api_generation_snapshot_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed40809d",
      "metadata": {},
      "source": [
        "### 0.1.7 Dataset snapshot — Installed generation capacities (RTE)\n",
        "\n",
        "This project also relies on **installed generation capacity datasets** published by RTE, which provide\n",
        "a **structural description of the French power system** by generation technology.\n",
        "\n",
        "Two complementary document types are used:\n",
        "\n",
        "- **Historical installed capacities by year**  \n",
        "  (e.g. `Capacite_installee_production_2022.csv`, available for several years)  \n",
        "  These files report installed capacity levels (in MW) by production technology for a given year.\n",
        "  The snapshot confirms:\n",
        "  - full coverage of major generation technologies (nuclear, wind, solar, hydro, thermal),\n",
        "  - annual time granularity with one capacity value per technology,\n",
        "  - consistent units (MW) across all years,\n",
        "  - smooth year-to-year evolution reflecting structural changes in the generation mix.\n",
        "\n",
        "- **Installed capacities for the current year**  \n",
        "  (e.g. `rte_installed_capacities_all_current_year.csv`)  \n",
        "  This dataset provides the **latest reference capacity levels** used for near real-time analysis.\n",
        "  The snapshot highlights:\n",
        "  - the same technology breakdown as in historical files,\n",
        "  - coherence between current-year capacities and the most recent historical observations,\n",
        "  - absence of temporal indexing, consistent with a static structural reference.\n",
        "\n",
        "Given their **small size and simple tabular structure**, these datasets are directly exploitable without\n",
        "additional preprocessing. They are used to contextualise realised production levels and to interpret\n",
        "grid stress indicators, rather than as direct time-series inputs to short-term forecasting models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.2 Top data issues\n",
        "\n",
        "\n",
        "Summarize any critical issues:\n",
        "- Missing timestamp blocks\n",
        "- DST anomalies (23h / 25h days)\n",
        "- High missing-rate variables\n",
        "- Extreme or suspicious values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.3 Key findings\n",
        "\n",
        "\n",
        "Write 3–5 high-level findings in plain language after completing the EDA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.4 Weekly mini-dashboard\n",
        "\n",
        "This section provides a compact visual overview of the French power system over a **representative week**.\n",
        "\n",
        "The figure combines:\n",
        "- national electricity **load**,\n",
        "- **population-weighted temperature**,\n",
        "- **wind** generation,\n",
        "- **solar** generation.\n",
        "\n",
        "All series are aligned on a common hourly time grid to highlight short-term interactions between demand, weather, and renewable production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5c629898",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/interim/consommation/consommation_2024_long.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m AGGLO_PATH = \u001b[33m\"\u001b[39m\u001b[33mdata/external/agglomerations.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m df_load = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m df_prod = pd.read_csv(PROD_YTD_PATH, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mstart_date\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mend_date\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m df_weather = pd.read_csv(WEATHER_PATH, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/interim/consommation/consommation_2024_long.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Paths (CSV only)\n",
        "# ----------------------------\n",
        "CONS_PATH = \"data/interim/consommation/consommation_2024_long.csv\"\n",
        "PROD_YTD_PATH = \"data/raw/production/rte_actual_generation_mix_15min_ytd_solar_wind.csv\"\n",
        "WEATHER_PATH = \"data/raw/weather/weather_32_cities_2024.csv\"\n",
        "AGGLO_PATH = \"data/external/agglomerations.xlsx\"\n",
        "\n",
        "# ----------------------------\n",
        "# Load data\n",
        "# ----------------------------\n",
        "df_load = pd.read_csv(CONS_PATH, parse_dates=[\"datetime\"])\n",
        "df_prod = pd.read_csv(PROD_YTD_PATH, parse_dates=[\"start_date\", \"end_date\"])\n",
        "df_weather = pd.read_csv(WEATHER_PATH, parse_dates=[\"date\"])\n",
        "df_weights = pd.read_excel(AGGLO_PATH)\n",
        "\n",
        "# ----------------------------\n",
        "# Select a representative week\n",
        "# (winter week with high load)\n",
        "# ----------------------------\n",
        "week_start = \"2024-01-15\"\n",
        "week_end = \"2024-01-22\"\n",
        "\n",
        "# ----------------------------\n",
        "# Load (30-min → hourly mean)\n",
        "# ----------------------------\n",
        "load_hourly = (\n",
        "    df_load\n",
        "    .set_index(\"datetime\")\n",
        "    .loc[week_start:week_end]\n",
        "    .resample(\"1H\")[\"load_mw\"]\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Renewable generation (15-min → hourly mean)\n",
        "# ----------------------------\n",
        "prod_hourly = (\n",
        "    df_prod\n",
        "    .set_index(\"start_date\")\n",
        "    .loc[week_start:week_end]\n",
        "    .groupby(\"production_type\")[\"value_mw\"]\n",
        "    .resample(\"1H\")\n",
        "    .mean()\n",
        "    .unstack(\"production_type\")\n",
        ")\n",
        "\n",
        "wind = prod_hourly[\"WIND\"]\n",
        "solar = prod_hourly[\"SOLAR\"]\n",
        "\n",
        "# ----------------------------\n",
        "# Population-weighted temperature\n",
        "# ----------------------------\n",
        "df_weather = df_weather.merge(\n",
        "    df_weights[[\"city\", \"population\"]],\n",
        "    on=\"city\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "df_weather[\"weighted_temp\"] = (\n",
        "    df_weather[\"temperature_2m\"] * df_weather[\"population\"]\n",
        ")\n",
        "\n",
        "temp_hourly = (\n",
        "    df_weather\n",
        "    .set_index(\"date\")\n",
        "    .loc[week_start:week_end]\n",
        "    .groupby(\"date\")\n",
        "    .agg(\n",
        "        weighted_temp=(\"weighted_temp\", \"sum\"),\n",
        "        population=(\"population\", \"sum\")\n",
        "    )\n",
        ")\n",
        "\n",
        "temp_hourly[\"temperature_fr\"] = (\n",
        "    temp_hourly[\"weighted_temp\"] / temp_hourly[\"population\"]\n",
        ")\n",
        "\n",
        "temperature = temp_hourly[\"temperature_fr\"]\n",
        "\n",
        "# ----------------------------\n",
        "# Plot: single figure\n",
        "# ----------------------------\n",
        "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Load\n",
        "ax1.plot(load_hourly.index, load_hourly, label=\"Load (MW)\")\n",
        "ax1.set_ylabel(\"Load (MW)\")\n",
        "ax1.set_xlabel(\"Date\")\n",
        "\n",
        "# Renewable production\n",
        "ax1.plot(wind.index, wind, label=\"Wind generation (MW)\")\n",
        "ax1.plot(solar.index, solar, label=\"Solar generation (MW)\")\n",
        "\n",
        "# Secondary axis for temperature\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(temperature.index, temperature, linestyle=\"--\", label=\"Temperature (°C)\")\n",
        "ax2.set_ylabel(\"Temperature (°C)\")\n",
        "\n",
        "# Legends\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper right\")\n",
        "\n",
        "plt.title(\"Weekly mini-dashboard — Load, Temperature, Wind & Solar\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data loading and reproducibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Data sources\n",
        "\n",
        "\n",
        "Document:\n",
        "- RTE : \n",
        "https://www.services-rte.com/en/download-data-published-by-rte.html?category=consumption&type=power_consumption\n",
        "\n",
        "https://www.services-rte.com/en/download-data-published-by-rte.html?category=generation&type=installed_capacities&subType=capacities_per_production_type (above 1MW)\n",
        "\n",
        "API for almost real time (yesterday until 00h) : https://data.rte-france.com/group/guest/apps/-/apps/2473352/smart-grid-project# (i will sent you the ID by mail for more safety) (or an account can be crated in two minutes)\n",
        "\n",
        "- Weather sources : \n",
        "https://open-meteo.com/en/docs/meteofrance-api for forecasts and\n",
        "https://open-meteo.com/en/docs/historical-weather-api for past years\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Load processed dataset\n",
        "\n",
        "\n",
        "Load the consolidated dataset.\n",
        "Show head / tail and basic shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Structural sanity checks\n",
        "\n",
        "\n",
        "Check:\n",
        "- Datetime index type and timezone\n",
        "- Sorted and unique timestamps\n",
        "- Inferred frequency\n",
        "- Column naming consistency and units\n",
        "- Coverage (min / max dates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Time axis integrity (global)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Frequency audit\n",
        "\n",
        "\n",
        "Analyze datetime differences:\n",
        "- Distribution of time deltas\n",
        "- Abnormal steps\n",
        "- Largest gaps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Missing timestamps\n",
        "\n",
        "\n",
        "- Build the expected full time grid\n",
        "- Count missing timestamps\n",
        "- Identify contiguous missing blocks (start / end / duration)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Daylight Saving Time (DST)\n",
        "\n",
        "\n",
        "- Identify 23-hour and 25-hour days\n",
        "- Decide and document the timezone policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Data quality and anomalies (global)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Missing values analysis\n",
        "\n",
        "\n",
        "- Missing rate per variable\n",
        "- Missingness over time (year / month)\n",
        "- Optional heatmap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Outlier detection\n",
        "\n",
        "\n",
        "- Extreme values in load and generation\n",
        "- Negative or impossible values\n",
        "- Flatlines and step changes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Known event windows\n",
        "\n",
        "\n",
        "Inspect known periods:\n",
        "- COVID shock\n",
        "- Extreme cold spells\n",
        "- Other major system events (if known)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A — Consumption (demand side)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. National electricity load: global behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Long-term trend\n",
        "\n",
        "\n",
        "- Multi-year load evolution\n",
        "- Rolling averages\n",
        "- Non-stationarity discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Load distribution and tails\n",
        "\n",
        "\n",
        "- Histogram / density\n",
        "- Quantiles\n",
        "- Extreme demand days\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Intra-day and intra-week patterns (load)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Daily profile\n",
        "\n",
        "\n",
        "- Average load by hour\n",
        "- Weekday vs weekend comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Weekly structure\n",
        "\n",
        "\n",
        "- Mean load by weekday\n",
        "- Hourly profiles for each weekday\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Seasonal patterns (load)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Monthly seasonality\n",
        "\n",
        "\n",
        "- Monthly averages\n",
        "- Monthly boxplots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Year-to-year comparison\n",
        "\n",
        "\n",
        "- Same-month comparisons across years\n",
        "- Structural break detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Thermo-sensitivity analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Load vs temperature\n",
        "\n",
        "\n",
        "- Scatter plots\n",
        "- Nonlinear smoothing\n",
        "- Winter vs summer asymmetry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Degree-days\n",
        "\n",
        "\n",
        "- Heating Degree Days (HDD)\n",
        "- Cooling Degree Days (CDD)\n",
        "- Comparison with raw temperature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Thermal inertia\n",
        "\n",
        "\n",
        "- Lagged or smoothed temperature features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part B — Production (supply side)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Electricity generation: overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.1 Total generation vs load\n",
        "\n",
        "\n",
        "- Compare total generation and load\n",
        "- Identify stress periods (imports/exports if available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2 Generation mix\n",
        "\n",
        "\n",
        "- Average mix by technology\n",
        "- Mix evolution over time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Production by technology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.1 Nuclear\n",
        "\n",
        "\n",
        "- Stability\n",
        "- Seasonal modulation\n",
        "- Ramp constraints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.2 Wind\n",
        "\n",
        "\n",
        "- Volatility\n",
        "- Seasonal behavior\n",
        "- Ramp rates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.3 Solar\n",
        "\n",
        "\n",
        "- Diurnal cycle\n",
        "- Seasonal amplitude\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.4 Hydro and thermal\n",
        "\n",
        "\n",
        "- Dispatchable role\n",
        "- Response during stress periods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Weather dependence (production)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.1 Wind vs wind speed\n",
        "\n",
        "\n",
        "- Scatter plots\n",
        "- Binned averages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Solar vs radiation\n",
        "\n",
        "\n",
        "- Scatter plots\n",
        "- Solar vs hour of day\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Seasonal weather effects\n",
        "\n",
        "\n",
        "- Winter vs summer comparisons\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Residual load and grid stress\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.1 Definition\n",
        "\n",
        "\n",
        "Residual load = load − (wind + solar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.2 Residual load analysis\n",
        "\n",
        "\n",
        "- Distribution\n",
        "- Daily profile\n",
        "- Extreme peaks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.3 Comparison with raw load\n",
        "\n",
        "\n",
        "- Tail behavior\n",
        "- Interpretation for grid stress\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross-cutting analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Calendar effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12.1 Weekday effects\n",
        "\n",
        "\n",
        "- Mean differences by weekday\n",
        "- Interaction with seasonality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12.2 Holidays and vacations\n",
        "\n",
        "\n",
        "- Holiday vs non-holiday comparison\n",
        "- Vacation-period effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13. Temporal dependence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.1 Autocorrelation\n",
        "\n",
        "\n",
        "- ACF / PACF for load and residual load\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.2 Lag correlations\n",
        "\n",
        "\n",
        "- Correlation at 24h, 48h, 168h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14. Feature relationships\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14.1 Correlation heatmaps\n",
        "\n",
        "\n",
        "- Load, weather, generation variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14.2 Redundancy checks\n",
        "\n",
        "\n",
        "- Multicollinearity inspection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 15. EDA synthesis and modeling decisions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15.1 Main conclusions\n",
        "\n",
        "\n",
        "Summarize confirmed patterns:\n",
        "- Seasonality\n",
        "- Nonlinearity\n",
        "- Intermittency\n",
        "- Temporal dependence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15.2 Modeling implications\n",
        "\n",
        "\n",
        "- Target choice\n",
        "- Feature set\n",
        "- Data cleaning policy\n",
        "- Backtesting strategy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
